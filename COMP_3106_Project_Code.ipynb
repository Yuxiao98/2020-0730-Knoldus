{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "COMP 3106 - Project Code.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# COMP 3106 Project Code \n",
        "\n",
        "## Authors \n",
        "Kyle Knobloch, Balreet Kaur Dhillon, Yuxiao Chen \n",
        "\n",
        "## Data\n",
        "\n",
        "Waste Water Data download: https://github.com/Big-Life-Lab/PHESD\n",
        "\n",
        "Ottawa Testin data: https://www.arcgis.com/home/item.html?id=26c902bf1da44d3d90b099392b544b81 \n",
        "\n",
        "## Notes\n",
        "I added stuff from the guide (https://realpython.com/python-ai-neural-network/#python-ai-starting-to-build-your-first-neural-network). \n",
        "\n",
        "Looks like there's a class that we could rely on heavily to do this work. - KYLE \n",
        "\n",
        "## TODO\n",
        "- [X] Data Compile \n",
        "- [ ] Get it to work with NeuralNetwork class from tutorial\n",
        "- [ ] Get it to work with the sklearn (proof of working) \n",
        "- [ ] Presentation with our findings "
      ],
      "metadata": {
        "id": "ZX6qaIo_HdEv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "muZ3HUK6HMxp",
        "outputId": "e044782d-02bd-42ed-baa5-38b294b69df5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['wastewater level' 'covid count positive on that day']\n",
            " ['0.00038713500000000004' '18.0']\n",
            " ['0.000173621' '31.0']\n",
            " ['0.000138118' '31.0']\n",
            " ['5.9589e-05' '21.0']\n",
            " ['9.7456e-05' '9.0']\n",
            " ['3.9861e-05' '2.0']\n",
            " ['4.9405000000000006e-05' '0.0']\n",
            " ['5.9341e-05' '10.0']\n",
            " ['4.0598999999999995e-05' '2.0']\n",
            " ['7.487100000000001e-05' '4.0']\n",
            " ['0.00032656899999999997' '1.0']\n",
            " ['0.00010175' '3.0']\n",
            " ['0.00010588700000000001' '1.0']\n",
            " ['0.000101206' '6.0']]\n",
            "Number of Records: 606\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import requests\n",
        "import matplotlib as plot\n",
        "from io import StringIO\n",
        "\n",
        "waster_water_url = 'https://raw.githubusercontent.com/Big-Life-Lab/PHESD/main/Wastewater/Ottawa/Data/wastewater_virus.csv'\n",
        "ww_r = requests.get(waster_water_url).text\n",
        "wastewater = np.genfromtxt(StringIO(ww_r), delimiter=\",\", dtype=None, encoding=None)\n",
        "#print(wastewater[:5])\n",
        "\n",
        "\n",
        "covid_testing_url = 'https://www.arcgis.com/sharing/rest/content/items/26c902bf1da44d3d90b099392b544b81/data'\n",
        "covid_r = requests.get(covid_testing_url).text\n",
        "covid_r = covid_r.replace('\"Date\",\"Number of Tests, Excluding LTCH\",\"Daily % Positivity, Excluding LTCH\",\"Number of Tests in LTCH\",\"LTCH Daily % Positivity\"', \n",
        "                          '\"Date\",\"Number of Tests Excluding LTCH\",\"Daily % Positivity Excluding LTCH\",\"Number of Tests in LTCH\",\"LTCH Daily % Positivity\"')\n",
        "covidtests = np.genfromtxt(StringIO(covid_r), delimiter=\",\", dtype=None, encoding=None)\n",
        "#print(covidtests[:5])\n",
        "\n",
        "covid_totals = []\n",
        "for i in covidtests:\n",
        "  if i[0] == '\"Date\"':\n",
        "    covid_totals.append([\"Total positive\"])\n",
        "  else:\n",
        "    covid_totals.append([int((float(i[1]) * float(i[2]) * 0.01) + (float(i[3]) * float(i[4]) * 0.01))])\n",
        "\n",
        "covidtests = np.append(covidtests, covid_totals, axis=1)\n",
        "#print(covidtests[:10])\n",
        "\n",
        "\n",
        "ww_covid = []\n",
        "# WITH DATE \n",
        "#ww_covid.append([\"date\", \"wastewater level\", \"covid count positive on that day\"])\n",
        "# NO DATE\n",
        "ww_covid.append([\"wastewater level\", \"covid count positive on that day\"])\n",
        "\n",
        "ww_count = 1 # ww count\n",
        "co_count = 1 # covid count \n",
        "while True: \n",
        "  if co_count >= len(covidtests) or ww_count >= len(wastewater):\n",
        "    break\n",
        "\n",
        "  # covid testing started before ww testing\n",
        "  #print(covidtests[co_count][0].replace('\"','') + \" \" + wastewater[ww_count][0])\n",
        "  if covidtests[co_count][0].replace('\"','') == wastewater[ww_count][0]:\n",
        "    # WITH DATE\n",
        "    #ww_covid.append([wastewater[ww_count][0], float(wastewater[ww_count][5]) + float(wastewater[ww_count][7]), float(covidtests[co_count][5])])\n",
        "    # NO DATE\n",
        "    ww_covid.append([float(wastewater[ww_count][5]) + float(wastewater[ww_count][7]), float(covidtests[co_count][5])])\n",
        "\n",
        "    ww_count = ww_count + 1\n",
        "  # Always increment \n",
        "  co_count = co_count + 1\n",
        "\n",
        "##########################\n",
        "### DATA #################\n",
        "#####################\n",
        "ww_covid = np.array(ww_covid)\n",
        "print(ww_covid[:15])\n",
        "print(\"Number of Records: \" + str(len(ww_covid)))\n",
        "\n",
        "\n",
        "# covN1: SARS-CoV-2 nucleocapsid gene N1 normalized with nPMMoV: Pepper mild mottle virus\n",
        "# covN2: SARS-CoV-2 nucleocapsid gene N2 normalized with nPMMoV: Pepper mild mottle virus\n",
        "# Added together gets the waste water value\n",
        "# https://github.com/Big-Life-Lab/PHES-ODM/blob/main/metadata_en.md\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_vector = [1.72, 1.23]\n",
        "weights_1 = [1.26, 0]\n",
        "weights_2 = [2.17, 0.33]\n",
        "\n",
        "first_indexes_mult = input_vector[0] * weights_1[0]\n",
        "second_indexes_mult = input_vector[1] * weights_1[1]\n",
        "dot_product_1 = first_indexes_mult + second_indexes_mult\n",
        "\n",
        "print(f\"The dot product is: {dot_product_1}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zF_eicQRjE5D",
        "outputId": "0f543900-a7e7-45a9-94af-6ce5ae71bf91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The dot product is: 2.1672\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dot_product_1 = np.dot(input_vector, weights_1)\n",
        "print(f\"The dot product is: {dot_product_1}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wsBmKzGujmEF",
        "outputId": "3a50fda8-3590-482d-cb6e-c12933444dfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The dot product is: 2.1672\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dot_product_2 = np.dot(input_vector, weights_2)\n",
        "print(f\"The dot product 2 is: {dot_product_2}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l0fUUphejt_t",
        "outputId": "d6f2bff7-a3fd-4f35-ed5c-7fce8723854a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The dot product 2 is: 4.1383\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_vector = np.array([1.66, 1.56])\n",
        "weights_1 = np.array([1.45, -0.66])\n",
        "bias = np.array([0.0])\n",
        "def sigmoid(x):\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def make_prediction(input_vector, weights, bias):\n",
        "  layer_1 = np.dot(input_vector, weights) + bias\n",
        "  layer_2 = sigmoid(layer_1)\n",
        "  return layer_2\n",
        "\n",
        "prediction = make_prediction(input_vector, weights_1, bias)\n",
        "print(f\"The prediction result is: {prediction}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNKE-OXxjzqh",
        "outputId": "ac141e2b-54d2-4404-a278-876e3c95ccc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The prediction result is: [0.7985731]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_vector = np.array([2, 1.5])\n",
        "prediction = make_prediction(input_vector, weights_1, bias)\n",
        "print(f\"The prediction result is: {prediction}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rAJhyTpjkVhJ",
        "outputId": "d95e8e1b-6fe5-4cef-9472-4a7981ce1c38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The prediction result is: [0.87101915]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "target = 0\n",
        "\n",
        "mse = np.square(prediction - target)\n",
        "print(f\"Prediction: {prediction}; Error: {mse}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPja7WMlkbw8",
        "outputId": "6d28c31f-489c-4685-b00d-5b37e9036e81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction: [0.87101915]; Error: [0.75867436]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "derivative = 2 * (prediction - target)\n",
        "print(f\"The derivative is {derivative}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-RH6OAf7km88",
        "outputId": "0bdd3467-2f3e-428e-efa0-8b0614d97d16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The derivative is [1.7420383]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " weights_1 = weights_1 - derivative\n",
        " prediction = make_prediction(input_vector, weights_1, bias)\n",
        " error = (prediction - target) ** 2\n",
        " print(f\"Prediction: {prediction}; Error: {error}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJwV11g7kmHM",
        "outputId": "9b05de70-eef4-4b0b-c74f-8b848bb6b3fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction: [0.01496248]; Error: [0.00022388]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CLASS\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, learning_rate):\n",
        "        self.weights = np.array([np.random.randn(), np.random.randn()])\n",
        "        self.bias = np.random.randn()\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "    def _sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def _sigmoid_deriv(self, x):\n",
        "        return self._sigmoid(x) * (1 - self._sigmoid(x))\n",
        "\n",
        "    def predict(self, input_vector):\n",
        "        layer_1 = np.dot(input_vector, self.weights) + self.bias\n",
        "        layer_2 = self._sigmoid(layer_1)\n",
        "        prediction = layer_2\n",
        "        return prediction\n",
        "\n",
        "    def _compute_gradients(self, input_vector, target):\n",
        "        layer_1 = np.dot(input_vector, self.weights) + self.bias\n",
        "        layer_2 = self._sigmoid(layer_1)\n",
        "        prediction = layer_2\n",
        "\n",
        "        derror_dprediction = 2 * (prediction - target)\n",
        "        dprediction_dlayer1 = self._sigmoid_deriv(layer_1)\n",
        "        dlayer1_dbias = 1\n",
        "        dlayer1_dweights = (0 * self.weights) + (1 * input_vector)\n",
        "\n",
        "        derror_dbias = (\n",
        "            derror_dprediction * dprediction_dlayer1 * dlayer1_dbias\n",
        "        )\n",
        "        derror_dweights = (\n",
        "            derror_dprediction * dprediction_dlayer1 * dlayer1_dweights\n",
        "        )\n",
        "\n",
        "        return derror_dbias, derror_dweights\n",
        "\n",
        "    def _update_parameters(self, derror_dbias, derror_dweights):\n",
        "        self.bias = self.bias - (derror_dbias * self.learning_rate)\n",
        "        self.weights = self.weights - (\n",
        "            derror_dweights * self.learning_rate\n",
        "        )\n",
        "    \n",
        "    def train(self, input_vectors, targets, iterations):\n",
        "        cumulative_errors = []\n",
        "        for current_iteration in range(iterations):\n",
        "            # Pick a data instance at random\n",
        "            random_data_index = np.random.randint(len(input_vectors))\n",
        "\n",
        "            input_vector = input_vectors[random_data_index]\n",
        "            target = targets[random_data_index]\n",
        "\n",
        "            # Compute the gradients and update the weights\n",
        "            derror_dbias, derror_dweights = self._compute_gradients(\n",
        "                input_vector, target\n",
        "            )\n",
        "\n",
        "            self._update_parameters(derror_dbias, derror_dweights)\n",
        "\n",
        "            # Measure the cumulative error for all the instances\n",
        "            if current_iteration % 100 == 0:\n",
        "                cumulative_error = 0\n",
        "                # Loop through all the instances to measure the error\n",
        "                for data_instance_index in range(len(input_vectors)):\n",
        "                    data_point = input_vectors[data_instance_index]\n",
        "                    target = targets[data_instance_index]\n",
        "\n",
        "                    prediction = self.predict(data_point)\n",
        "                    error = np.square(prediction - target)\n",
        "\n",
        "                    cumulative_error = cumulative_error + error\n",
        "                cumulative_errors.append(cumulative_error)\n",
        "\n",
        "        return cumulative_errors\n",
        "\n",
        "# MAIN\n",
        "learning_rate = 0.1\n",
        "\n",
        "neural_network = NeuralNetwork(learning_rate)\n",
        "\n",
        "neural_network.predict(input_vector)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8BsCqrUnM1Y",
        "outputId": "77ac2ffd-ed56-4352-9b0f-5d1ba8bfeba0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.00042518255387658985"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x = np.arange(1, 25).reshape(12, 2)\n",
        "y = np.array([0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0])\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=4, random_state=4)\n",
        "x_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzi86_lYt-rN",
        "outputId": "13f365a5-b89c-4fcf-c00b-ca95836768ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[17, 18],\n",
              "       [ 5,  6],\n",
              "       [23, 24],\n",
              "       [ 1,  2],\n",
              "       [ 3,  4],\n",
              "       [11, 12],\n",
              "       [15, 16],\n",
              "       [21, 22]])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    }
  ]
}